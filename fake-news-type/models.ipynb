{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWQRxmPdRxfN"
   },
   "source": [
    "# **Training DNN and Bi-LSTM Models for fake news type classification**\n",
    "\n",
    "Two datasets:\n",
    "\n",
    "1. All Fakes = 1:\"Disinformation\", 2:\"Hoax\", 3:\"Propaganda\", 4:\"Trusted\"\n",
    "\n",
    "2. TOVS =  1:\"Satire\", 2:\"Hoax\", 3:\"Propaganda\", 4:\"Trusted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "-LjH0Mpga4eo"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "#import tensorflow_addons as tfa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "PHbG8l4xQ61B"
   },
   "outputs": [],
   "source": [
    "intermed_path = 'intermediate/'\n",
    "model_path = 'model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0XnBQydRD4b",
    "outputId": "363903df-0b4d-461f-cb7d-306a411b32a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3072, 1000)\n",
      "4\n",
      "(3072, 4)\n"
     ]
    }
   ],
   "source": [
    "## Load features and labels\n",
    "## To load TOVS features and labels, replace the prefix 'allfakes' with 'tovs'\n",
    "data = np.load(os.path.join(intermed_path, 'allfakes_feat.npz'))\n",
    "label = np.load(os.path.join(intermed_path, 'allfakes_lb.npz'))\n",
    "data = data['arr_0']\n",
    "label = label['arr_0']\n",
    "\n",
    "with open(os.path.join(intermed_path ,'allfakes_lb.pkl'), 'rb') as handle:\n",
    "       mlb = pickle.load(handle)\n",
    "num_label = mlb.classes_.shape[0]\n",
    "\n",
    "print(data.shape)\n",
    "print(num_label)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAJNe7o1cjhw"
   },
   "source": [
    "## **TRAIN DNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TYp4Z3BEbRhg"
   },
   "outputs": [],
   "source": [
    "def get_in_layer(x, dim_from, dim_to):\n",
    "   in_layer = tf.keras.layers.Lambda(\n",
    "       lambda x: x[:, dim_from:dim_to, :])(x)\n",
    "\n",
    "   return tf.keras.layers.GlobalAveragePooling1D()(in_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ybHa3erTcnpp"
   },
   "outputs": [],
   "source": [
    "def dnn(input_dim, num_labels, vocab_size = 300000, embedding_dim = 300):\n",
    "   max_len = sum(input_dim)\n",
    "   inputs = tf.keras.Input(shape=(max_len,), name='input')\n",
    "\n",
    "   x = tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                input_length = max_len,\n",
    "                                name = 'embeddings')(inputs)\n",
    "   in_layers = []\n",
    "   in_start = 0\n",
    "   for in_len in input_dim:\n",
    "       in_layers.append(get_in_layer(x, in_start, in_start + in_len)) #Calling function for global average pooling\n",
    "       in_start += in_len    \n",
    "\n",
    "   #x = tf.keras.layers.concatenate(in_layers) #Concatenating all pooled input embeddings\n",
    "   x = in_layers.pop()\n",
    "   \n",
    "   x = tf.keras.layers.Dropout(0.2)(x)\n",
    "   x = tf.keras.layers.Dense(1024)(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   x = tf.keras.layers.Activation('relu')(x)\n",
    "   x = tf.keras.layers.Dropout(0.2)(x)\n",
    "   x = tf.keras.layers.Dense(512)(x)\n",
    "   x = tf.keras.layers.BatchNormalization()(x)\n",
    "   x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "   x = tf.keras.layers.Dropout(0.2)(x)\n",
    "   output_name = 'output'\n",
    "   activation_function = 'softmax'\n",
    "   output_loss = 'categorical_crossentropy'\n",
    "   output_metric = ['accuracy']\n",
    "\n",
    "   x_type = tf.keras.layers.Dense(num_labels)(x)\n",
    "   x_type = tf.keras.layers.BatchNormalization()(x_type)\n",
    "   x_type = tf.keras.layers.Activation(\n",
    "               activation_function,\n",
    "               name=output_name)(x_type)\n",
    "   output_layers = x_type\n",
    "\n",
    "   #adam = tf.contrib.opt.LazyAdamOptimizer(learning_rate=0.01, beta1=0.9, beta2=0.999)\n",
    "   model = tf.keras.Model(inputs=inputs, outputs=output_layers)\n",
    "   model.compile(loss=output_loss, optimizer='adam', metrics=output_metric)\n",
    "\n",
    "   return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "UkCmWRHrbnrR"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "vocab_size = 300000\n",
    "embedding_dim = 100\n",
    "input_dim = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_WfD583dbdT",
    "outputId": "3ebe2696-57ef-48e1-c376-3862a6f5b40f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "96/96 [==============================] - 17s 169ms/step - loss: 1.2784 - accuracy: 0.4530\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 16s 167ms/step - loss: 0.6537 - accuracy: 0.8003\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 16s 164ms/step - loss: 0.3693 - accuracy: 0.9507\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 15s 160ms/step - loss: 0.2822 - accuracy: 0.9706\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 15s 159ms/step - loss: 0.2392 - accuracy: 0.9756\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 15s 160ms/step - loss: 0.2044 - accuracy: 0.9811\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 15s 159ms/step - loss: 0.1753 - accuracy: 0.9856\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 15s 159ms/step - loss: 0.1526 - accuracy: 0.9888\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 15s 160ms/step - loss: 0.1338 - accuracy: 0.9921\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 15s 159ms/step - loss: 0.1210 - accuracy: 0.9892\n"
     ]
    }
   ],
   "source": [
    "model = dnn(input_dim, num_label, vocab_size, embedding_dim)\n",
    "\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto')\n",
    "model.fit(x = data, y = label, batch_size = batch_size, epochs = epochs, shuffle=True)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "       model,\n",
    "       filepath = os.path.join(model_path, 'dnn_model_weight_allfakes.h5'),\n",
    "       overwrite=True,\n",
    "       )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ru4ZmlDLRb6Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIWU52E4TCZX"
   },
   "source": [
    "## **TRAIN LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "HuOOwzDnTEKX"
   },
   "outputs": [],
   "source": [
    "def get_lstm(input_dim, num_labels, vocab_size = 300000, embedding_dim = 300):\n",
    "\n",
    "  max_len = sum(input_dim)\n",
    "  inputs = tf.keras.Input(shape=(max_len,), name='lstm_input')\n",
    "\n",
    "  x = tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                input_length = max_len,\n",
    "                                name = 'embeddings')(inputs)\n",
    "  \n",
    "  forward_sent = tf.keras.layers.LSTM(512, dropout = 0.2)(x)\n",
    "  backward_sent = tf.keras.layers.LSTM(512, dropout=0.2, go_backwards=True)(x)\n",
    "  sent_encode = tf.keras.layers.Concatenate()([forward_sent, backward_sent])\n",
    "  sent_encode = tf.keras.layers.Dropout(0.2)(sent_encode)\n",
    "  output = tf.keras.layers.Dense(num_labels, activation='softmax')(sent_encode)\n",
    "\n",
    "  model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  print(model.summary())\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Wok797AQTGD9"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "vocab_size = 300000\n",
    "embedding_dim = 100\n",
    "input_dim = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIO7B7rBTPgi",
    "outputId": "bbdb8319-c11c-4d35-b76f-55d6ed69f809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "lstm_input (InputLayer)         [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embeddings (Embedding)          (None, 1000, 100)    30000000    lstm_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 512)          1255424     embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "lstm_12 (LSTM)                  (None, 512)          1255424     embeddings[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1024)         0           lstm_11[0][0]                    \n",
      "                                                                 lstm_12[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 1024)         0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 4)            4100        dropout_14[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 32,514,948\n",
      "Trainable params: 32,514,948\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "96/96 [==============================] - 41s 407ms/step - loss: 1.4164 - accuracy: 0.3443\n",
      "Epoch 2/10\n",
      "96/96 [==============================] - 39s 407ms/step - loss: 0.8662 - accuracy: 0.6795\n",
      "Epoch 3/10\n",
      "96/96 [==============================] - 39s 407ms/step - loss: 0.3360 - accuracy: 0.9030\n",
      "Epoch 4/10\n",
      "96/96 [==============================] - 39s 409ms/step - loss: 0.0903 - accuracy: 0.9746\n",
      "Epoch 5/10\n",
      "96/96 [==============================] - 39s 409ms/step - loss: 0.0216 - accuracy: 0.9943\n",
      "Epoch 6/10\n",
      "96/96 [==============================] - 39s 408ms/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "96/96 [==============================] - 39s 409ms/step - loss: 0.0176 - accuracy: 0.9958\n",
      "Epoch 8/10\n",
      "96/96 [==============================] - 39s 408ms/step - loss: 0.0076 - accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "96/96 [==============================] - 39s 408ms/step - loss: 0.0142 - accuracy: 0.9947\n",
      "Epoch 10/10\n",
      "96/96 [==============================] - 39s 408ms/step - loss: 0.0016 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = get_lstm(input_dim, num_label, vocab_size, embedding_dim)\n",
    "\n",
    "earlystop_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, verbose=1, mode='auto')\n",
    "model.fit(x = data, y = label, batch_size = batch_size, epochs = epochs, shuffle=True)\n",
    "\n",
    "tf.keras.models.save_model(\n",
    "       model,\n",
    "       filepath = os.path.join(model_path, 'lstm_model_weight_allfakes.h5'),\n",
    "       overwrite=True,\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-WmLAVQXVzu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "all-fakes-models.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
